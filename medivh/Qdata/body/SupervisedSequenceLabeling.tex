\chapter{Supervised Sequence Labelling}
This chapter provides the background material and literature review for supervised sequence labelling. Section 2.1 briefly reviews supervised learning in general. Section 2.2 covers the classical, non-sequential framework of supervised pattern classication. Section 2.3 denes supervised sequence labelling, and describes the different classes of sequence labelling task that arise under different assumptions about the label sequences.

\section{Supervised Learning}
Machine learning problems where a set of input-target pairs is provided for training are referred to as supervised learning tasks. This is distinct from reinforcement learning, where only scalar reward values are provided for training, and unsupervised learning, where no training signal exists at all, and the algorithm attempts to uncover the structure of the data by inspection alone. We will not consider either reinforcement learning or unsupervised learning in this book.

A supervised learning task consists of a training set S of input-target pairs $(x, z)$, where $x$ is an element of the input space $\mathcal{X}$ and $z$ is an element of the target space $\mathcal{Z}$, along with a disjoint test set $\mathcal{S}'$. We will sometimes refer to the elements of $\mathcal{S}$ as training examples. Both $\mathcal{S}$ and $\mathcal{S}'$ are assumed to have been drawn independently from the same input-target distribution $\mathcal{D}_{\mathcal{X}\times\mathcal{Z}}$. In some cases an extra validation set is drawn from the training set to validate the performance of the learning algorithm during training; in particular validation sets are frequently used to determine when training should stop, in order to prevent overfitting. The goal is to use the training set to minimise some taskspeci c error measure $E$ defined on the test set. For example, in a regression task, the usual error measure is the sum-of-squares, or squared Euclidean distance between the algorithm outputs and the test-set targets. For parametric algorithms (such as neural networks) the usual approach to error minimisation is to incrementally adjust the algorithm parameters to optimise a loss function on the training set, which is as closely related as possible to $E$. The transfer of learning from the training set to the test set is known as generalisation, and will be discussed further in later chapters.

The nature and degree of supervision provided by the targets varies greatly between supervised learning tasks. For example, training a supervised learner to correctly label every pixel corresponding to an aeroplane in an image requires a much more informative target than simply training it recognise whether or not an aeroplane is present. To distinguish these extremes, people sometimes refer to weakly and strongly labelled data.

\section{Pattern Classification}
Pattern classification, also known as pattern recognition, is one of the most extensively studied areas of machine learning (Bishop, 2006; Duda et al., 2000), and certain pattern classifiers, such as multilayer perceptrons (Rumelhart et al., 1986; Bishop, 1995) and support vector machines (Vapnik, 1995) have become familiar to the scientific community at large. Although pattern classification deals with non-sequential data, much of the practical and theoretical framework underlying it carries over to the sequential case. It is therefore instructive to brie y review this framework before we turn to sequence labelling.
